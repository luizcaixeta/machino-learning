---
title: Perceptron Redux
date: 2024-11-01
summary: Revisão do Perceptron simples e por que ainda importa na era dos Transformers.
---

## Introdução

O **Perceptron** é um dos modelos mais simples em *machine learning*, mas ainda é uma excelente porta de entrada para entender:

- Como modelos lineares funcionam  
- O papel de pesos e bias  
- Limitações de separabilidade linear  

## Equação do modelo

Um perceptron binário clássico pode ser escrito como:

$$
\hat{y} = \operatorname{sign}(w^T x + b)
$$

onde:

- $x \in \mathbb{R}^d$ é o vetor de entrada  
- $w$ são os pesos  
- $b$ é o bias  

## Gráfico / figura

Você pode salvar figuras geradas em Python/Matplotlib, por exemplo, em `public/images/` e referenciá-las assim:

![Fronteira de decisão do perceptron](/images/perceptron-decision-boundary.png)

Se a imagem não existir ainda, basta criá-la depois no diretório `public/images/`.

## Código em bloco

```python
import numpy as np

def perceptron_step(x, y, w, b, lr=0.1):
    y_hat = 1 if (np.dot(w, x) + b) >= 0 else -1
    if y_hat != y:
        w = w + lr * y * x
        b = b + lr * y
    return w, b
```

## Conclusão

Mesmo sendo um modelo simples, o Perceptron ajuda a construir intuição para arquiteturas mais complexas, como redes neurais profundas e Transformers.
